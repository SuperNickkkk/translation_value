# -*- coding: utf-8 -*-
"""
é£æœºç»´ä¿®ç¿»è¯‘è¯„ä¼°ç³»ç»Ÿ - Webç‰ˆæœ¬ä¸»åº”ç”¨
ä¸“ä¸šçš„Webç•Œé¢ï¼Œé›†æˆæ¨¡å‹é€‰æ‹©ã€æµ‹è¯•ç»“æœã€è¯„ä¼°å±•ç¤ºäºä¸€ä½“
"""

import os
import sys
import json
import time
import logging
from datetime import datetime
from pathlib import Path
# æ·»åŠ Qwen APIå®¢æˆ·ç«¯æ”¯æŒ
sys.path.append(str(Path(__file__).parent.parent))
from qwen_api_client import QwenAPIClient

from flask import Flask, render_template, request, jsonify, session, flash, redirect, url_for
from dotenv import load_dotenv
from flask_sqlalchemy import SQLAlchemy
from werkzeug.utils import secure_filename
import asyncio
from threading import Thread
import uuid

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥æ ¸å¿ƒæ¨¡å—
sys.path.append(str(Path(__file__).parent.parent))

from translation_evaluation_config import TranslationEvaluationConfig, ModelConfig
from translation_data_manager import TranslationDataManager, TranslationPair
from translation_engine import SpecializedTranslationEngine
from evaluation_engine import EvaluationEngine
from local_model_manager import local_model_manager, initialize_local_models

# å¯¼å…¥æ–°çš„ç›‘æ§å·¥å…·
from web_app.utils.performance_monitor import performance_monitor
from utils.log_manager import log_manager

app = Flask(__name__)
app.config['SECRET_KEY'] = 'aviation-translation-evaluation-secret-key'
app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///aviation_translation.db'
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False
app.config['UPLOAD_FOLDER'] = 'uploads'
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB

db = SQLAlchemy(app)

# å…¨å±€å˜é‡å­˜å‚¨ç³»ç»Ÿç»„ä»¶
translation_system = None
config_manager = None
data_manager = None
translation_engine = None
evaluation_engine = None

# ä»»åŠ¡çŠ¶æ€å­˜å‚¨å’Œæ§åˆ¶
active_tasks = {}
task_control_flags = {}  # ä»»åŠ¡æ§åˆ¶æ ‡å¿—ï¼š{task_id: {'paused': bool, 'terminated': bool}}

class EvaluationTask(db.Model):
    """è¯„ä¼°ä»»åŠ¡æ•°æ®æ¨¡å‹"""
    id = db.Column(db.String(36), primary_key=True)
    name = db.Column(db.String(200), nullable=False)
    status = db.Column(db.String(20), default='pending')  # pending, running, completed, failed
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    completed_at = db.Column(db.DateTime)
    data_file = db.Column(db.String(500))
    results_file = db.Column(db.String(500))
    progress = db.Column(db.Integer, default=0)
    total_pairs = db.Column(db.Integer, default=0)
    error_message = db.Column(db.Text)

class TranslationResult(db.Model):
    """ç¿»è¯‘ç»“æœæ•°æ®æ¨¡å‹"""
    id = db.Column(db.Integer, primary_key=True)
    task_id = db.Column(db.String(36), db.ForeignKey('evaluation_task.id'), nullable=False)
    pair_id = db.Column(db.String(100), nullable=False)
    source_text = db.Column(db.Text, nullable=False)
    target_text = db.Column(db.Text, nullable=False)
    model_name = db.Column(db.String(100), nullable=False)
    translated_text = db.Column(db.Text)
    accuracy_score = db.Column(db.Float)
    fluency_score = db.Column(db.Float)
    terminology_score = db.Column(db.Float)
    overall_score = db.Column(db.Float)
    evaluation_details = db.Column(db.Text)  # JSONæ ¼å¼çš„è¯¦ç»†è¯„ä¼°ä¿¡æ¯

def initialize_system():
    """åˆå§‹åŒ–ç¿»è¯‘è¯„ä¼°ç³»ç»Ÿ"""
    global translation_system, config_manager, data_manager, translation_engine, evaluation_engine
    
    try:
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        os.makedirs('uploads', exist_ok=True)
        os.makedirs('results', exist_ok=True)
        os.makedirs('logs', exist_ok=True)
        
        # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        config_file = Path(__file__).parent.parent / 'translation_config.json'
        config_manager = TranslationEvaluationConfig(str(config_file))
        if config_file.exists():
            config_manager.load_config()

        # åŠ è½½ .envï¼ˆä½¿ç”¨é¡¹ç›®æ ¹ç›®å½• .envï¼‰ï¼Œå¹¶ç”¨ç¯å¢ƒå˜é‡è¦†ç›–åœ¨çº¿æ¨¡å‹ API Keyï¼ˆæŒä¹…åŒ–åˆ°é…ç½®æ–‡ä»¶ï¼‰
        try:
            dotenv_path = Path(__file__).parent.parent / '.env'
            load_dotenv(dotenv_path=dotenv_path)
            ernie_key = os.getenv('ERNIE_API_KEY', '').strip()
            qwen_key = os.getenv('QWEN_API_KEY', '').strip()
            qwen_base = os.getenv('QWEN_BASE_URL', '').strip()
            qwen_model = os.getenv('QWEN_MODEL_ID', '').strip()
            changed = False
            # è¦†ç›–åœ¨çº¿æ¨¡å‹ Keyï¼ˆè‹¥æä¾›ï¼‰
            if 'ernie-4.5-0.3b' in config_manager.translation_models and ernie_key:
                config_manager.translation_models['ernie-4.5-0.3b'].api_key = ernie_key
                changed = True
            if 'qwen3-8b' in config_manager.translation_models:
                if (qwen_key or ernie_key):
                    # å…¼å®¹åŒä¸€å¹³å° Key å¤ç”¨ï¼ˆè‹¥ QWEN_API_KEY æœªæä¾›åˆ™å›è½åˆ° ERNIE_API_KEYï¼‰
                    config_manager.translation_models['qwen3-8b'].api_key = (qwen_key or ernie_key)
                    changed = True
                if qwen_base:
                    config_manager.translation_models['qwen3-8b'].base_url = qwen_base
                    changed = True
                if qwen_model:
                    config_manager.translation_models['qwen3-8b'].model_id = qwen_model
                    changed = True
            # è¦†ç›–è¯„ä¼°æ¨¡å‹ Keyï¼ˆè‹¥æä¾› ERNIE_API_KEYï¼‰
            if config_manager.evaluation_model and ernie_key:
                config_manager.evaluation_model.api_key = ernie_key
                changed = True
            if changed:
                config_manager.save_config()
        except Exception as _e:
            logging.warning(f"ç¯å¢ƒå˜é‡è¦†ç›–é…ç½®å¤±è´¥: {_e}")
        
        # åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
        data_manager = TranslationDataManager()
        
        # åˆå§‹åŒ–ç¿»è¯‘å¼•æ“
        translation_engine = SpecializedTranslationEngine()
        
        # åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹
        initialize_local_models()
        
        # åˆå§‹åŒ–è¯„ä¼°å¼•æ“ (ç¨åé…ç½®)
        evaluation_engine = None
        
        # å¯åŠ¨æ€§èƒ½ç›‘æ§
        performance_monitor.start_monitoring()
        # æ³¨å†Œä¸¤ä¸ªæœ¬åœ°æ¨¡å‹ç«¯ç‚¹ç”¨äºç›‘æ§
        performance_monitor.register_local_model('gemma-3-270m', 'http://127.0.0.1:8081')
        performance_monitor.register_local_model('qwen2.5-0.5b-instruct', 'http://127.0.0.1:8082')
        performance_monitor.register_local_model('qwen3-0.6b', 'http://127.0.0.1:8084')
        # æ–°å¢ï¼šæ³¨å†Œ ERNIE-4.5-0.3B-PT æœ¬åœ°æœåŠ¡ï¼ˆMPSï¼‰
        performance_monitor.register_local_model('ernie-4.5-0.3b-pt', 'http://127.0.0.1:8083')
        
        # å°è¯•é¢„å¯åŠ¨æœ¬åœ°æ¨¡å‹ï¼ˆé¿å…é¦–æ¬¡è°ƒç”¨æ—¶å¯åŠ¨å¤±è´¥ï¼‰
        try:
            from local_model_manager import local_model_manager as _mgr
            _mgr.start_model("qwen2.5-0.5b-instruct")
            _mgr.start_model("ernie-4.5-0.3b-pt")
            _mgr.start_model("gemma-3-270m")
            _mgr.start_model("qwen3-0.6b")
        except Exception as _e:
            logging.warning(f"é¢„å¯åŠ¨æœ¬åœ°æ¨¡å‹å¤±è´¥: {_e}")

        # ç§»é™¤ç¤ºä¾‹æ€§èƒ½æ•°æ®ï¼Œæ”¹ä¸ºä»…è®°å½•çœŸå®è°ƒç”¨
        
        # åˆ›å»ºæ•°æ®åº“è¡¨
        with app.app_context():
            db.create_all()
        
        logging.info("Webç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        return True
        
    except Exception as e:
        logging.error(f"ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        return False

@app.route('/')
def index():
    """ä¸»é¡µ - é›†æˆæ‰€æœ‰åŠŸèƒ½çš„å•é¡µé¢åº”ç”¨"""
    # è·å–å¯ç”¨çš„ç¿»è¯‘æ¨¡å‹
    available_models = []
    if config_manager:
        for model_key, model_config in config_manager.translation_models.items():
            available_models.append({
                'key': model_key,
                'name': model_config.name,
                'configured': bool(model_config.api_key)
            })
    
    # è·å–è¯„ä¼°æ¨¡å‹çŠ¶æ€
    evaluation_model_configured = False
    if config_manager and config_manager.evaluation_model:
        evaluation_model_configured = bool(config_manager.evaluation_model.api_key)
    
    # è·å–æœ€è¿‘çš„ä»»åŠ¡
    recent_tasks = EvaluationTask.query.order_by(EvaluationTask.created_at.desc()).limit(5).all()
    
    return render_template('index.html', 
                         available_models=available_models,
                         evaluation_model_configured=evaluation_model_configured,
                         recent_tasks=recent_tasks)

@app.route('/api/models', methods=['GET', 'POST'])
def manage_models():
    """ç®¡ç†ç¿»è¯‘æ¨¡å‹API"""
    if request.method == 'GET':
        # è¿”å›æ¨¡å‹é…ç½®ä¿¡æ¯
        models = {}
        if config_manager:
            for model_key, model_config in config_manager.translation_models.items():
                models[model_key] = {
                    'name': model_config.name,
                    'configured': bool(model_config.api_key),
                    'base_url': model_config.base_url,
                    'model_id': model_config.model_id
                }
        return jsonify(models)
    
    elif request.method == 'POST':
        # æ·»åŠ æˆ–æ›´æ–°æ¨¡å‹é…ç½®
        data = request.get_json()
        model_key = data.get('model_key')
        
        if not model_key:
            return jsonify({'error': 'ç¼ºå°‘æ¨¡å‹æ ‡è¯†'}), 400
        
        try:
            # ä½¿ç”¨é…ç½®ç®¡ç†å™¨çš„æ–¹æ³•æ¥æ·»åŠ æ¨¡å‹
            success = config_manager.add_translation_model(
                model_key=model_key,
                api_key=data.get('api_key', ''),
                name=data.get('name', ''),
                base_url=data.get('base_url', ''),
                model_id=data.get('model_id', ''),
                temperature=data.get('temperature', 0.3),
                max_tokens=data.get('max_tokens', 1000)
            )
            
            if not success:
                return jsonify({'error': f'ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_key}'}), 400
            
            # ä¿å­˜é…ç½®åˆ°æ–‡ä»¶
            config_manager.save_config()
            
            return jsonify({'message': 'æ¨¡å‹é…ç½®å·²ä¿å­˜', 'success': True})
            
        except Exception as e:
            return jsonify({'error': f'ä¿å­˜é…ç½®å¤±è´¥: {str(e)}'}), 500

@app.route('/api/evaluation_model', methods=['GET', 'POST'])
def manage_evaluation_model():
    """ç®¡ç†è¯„ä¼°æ¨¡å‹API"""
    if request.method == 'GET':
        if config_manager and config_manager.evaluation_model:
            return jsonify({
                'name': config_manager.evaluation_model.name,
                'configured': bool(config_manager.evaluation_model.api_key),
                'base_url': config_manager.evaluation_model.base_url,
                'model_id': config_manager.evaluation_model.model_id
            })
        return jsonify({'configured': False})
    
    elif request.method == 'POST':
        data = request.get_json()
        
        try:
            # ä½¿ç”¨é…ç½®ç®¡ç†å™¨çš„æ–¹æ³•æ¥æ›´æ–°è¯„ä¼°æ¨¡å‹
            success = config_manager.update_evaluation_model(
                api_key=data.get('api_key', ''),
                name=data.get('name', 'ERNIEè¯„ä¼°æ¨¡å‹'),
                base_url=data.get('base_url', ''),
                model_id=data.get('model_id', ''),
                temperature=data.get('temperature', 0.3),
                max_tokens=data.get('max_tokens', 1000)
            )
            
            if not success:
                return jsonify({'error': 'æ›´æ–°è¯„ä¼°æ¨¡å‹å¤±è´¥'}), 500
            
            # ä¿å­˜é…ç½®åˆ°æ–‡ä»¶
            config_manager.save_config()
            
            return jsonify({'message': 'è¯„ä¼°æ¨¡å‹é…ç½®å·²ä¿å­˜', 'success': True})
            
        except Exception as e:
            return jsonify({'error': f'ä¿å­˜é…ç½®å¤±è´¥: {str(e)}'}), 500

@app.route('/api/upload', methods=['POST'])
def upload_file():
    """ä¸Šä¼ ç¿»è¯‘æ•°æ®æ–‡ä»¶"""
    if 'file' not in request.files:
        return jsonify({'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400
    
    if file and file.filename.lower().endswith('.json'):
        try:
            filename = secure_filename(file.filename)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"{timestamp}_{filename}"
            filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
            file.save(filepath)
            
            # éªŒè¯JSONæ ¼å¼
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # éªŒè¯æ•°æ®æ ¼å¼
            if 'translation_pairs' not in data:
                return jsonify({'error': 'JSONæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘translation_pairså­—æ®µ'}), 400
            
            pairs_count = len(data['translation_pairs'])
            
            return jsonify({
                'message': 'æ–‡ä»¶ä¸Šä¼ æˆåŠŸ',
                'filename': filename,
                'filepath': filepath,
                'pairs_count': pairs_count,
                'success': True
            })
            
        except json.JSONDecodeError:
            return jsonify({'error': 'JSONæ–‡ä»¶æ ¼å¼é”™è¯¯'}), 400
        except Exception as e:
            return jsonify({'error': f'æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}'}), 500
    
    return jsonify({'error': 'åªæ”¯æŒJSONæ ¼å¼æ–‡ä»¶'}), 400

@app.route('/api/evaluate', methods=['POST'])
def start_evaluation():
    """å¼€å§‹ç¿»è¯‘è¯„ä¼°ä»»åŠ¡"""
    data = request.get_json()
    
    if not data:
        return jsonify({'error': 'ç¼ºå°‘è¯·æ±‚æ•°æ®'}), 400
    
    filepath = data.get('filepath')
    task_name = data.get('task_name', f'è¯„ä¼°ä»»åŠ¡_{datetime.now().strftime("%Y%m%d_%H%M%S")}')
    selected_models = data.get('selected_models', [])
    
    # æ•°æ®é€‰æ‹©å‚æ•°
    data_selection = data.get('data_selection', {
        'mode': 'all',
        'value': None
    })
    
    if not filepath or not os.path.exists(filepath):
        return jsonify({'error': 'æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨'}), 400
    
    if not selected_models:
        return jsonify({'error': 'è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªç¿»è¯‘æ¨¡å‹'}), 400
    
    # æ£€æŸ¥è¯„ä¼°æ¨¡å‹æ˜¯å¦é…ç½®
    if not config_manager or not config_manager.evaluation_model or not config_manager.evaluation_model.api_key:
        return jsonify({'error': 'è¯„ä¼°æ¨¡å‹æœªé…ç½®ï¼Œè¯·å…ˆé…ç½®è¯„ä¼°æ¨¡å‹'}), 400
    
    try:
        # åˆ›å»ºè¯„ä¼°ä»»åŠ¡
        task_id = str(uuid.uuid4())
        task = EvaluationTask(
            id=task_id,
            name=task_name,
            status='pending',
            data_file=filepath
        )
        
        # è¯»å–æ•°æ®æ–‡ä»¶è·å–æ€»æ•°
        with open(filepath, 'r', encoding='utf-8') as f:
            data_content = json.load(f)
            task.total_pairs = len(data_content.get('translation_pairs', []))
        
        db.session.add(task)
        db.session.commit()
        
        # å¯åŠ¨åå°è¯„ä¼°ä»»åŠ¡
        thread = Thread(target=run_evaluation_task, args=(task_id, filepath, selected_models, data_selection))
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'message': 'è¯„ä¼°ä»»åŠ¡å·²å¯åŠ¨',
            'task_id': task_id,
            'success': True
        })
        
    except Exception as e:
        return jsonify({'error': f'å¯åŠ¨è¯„ä¼°ä»»åŠ¡å¤±è´¥: {str(e)}'}), 500

def apply_data_selection(translation_pairs, data_selection):
    """æ ¹æ®æ•°æ®é€‰æ‹©å‚æ•°ç­›é€‰ç¿»è¯‘å¯¹æ•°æ®"""
    if not data_selection or data_selection.get('mode') == 'all':
        return translation_pairs
    
    mode = data_selection.get('mode')
    value = data_selection.get('value')
    
    if not value or value <= 0:
        return translation_pairs
    
    total_pairs = len(translation_pairs)
    
    if mode == 'percentage':
        # æŒ‰ç™¾åˆ†æ¯”é€‰æ‹©
        percentage = min(100, max(1, value))  # ç¡®ä¿åœ¨1-100èŒƒå›´å†…
        selected_count = max(1, int(total_pairs * percentage / 100))
        return translation_pairs[:selected_count]
    
    elif mode == 'count':
        # æŒ‰æ•°é‡é€‰æ‹©
        selected_count = min(total_pairs, max(1, int(value)))  # ä¸è¶…è¿‡æ€»æ•°
        return translation_pairs[:selected_count]
    
    return translation_pairs

def run_evaluation_task(task_id, filepath, selected_models, data_selection=None):
    """è¿è¡Œè¯„ä¼°ä»»åŠ¡çš„åå°å‡½æ•°"""
    with app.app_context():
        task = EvaluationTask.query.get(task_id)
        if not task:
            return
        
        # åˆå§‹åŒ–ä»»åŠ¡æ§åˆ¶æ ‡å¿—
        task_control_flags[task_id] = {'paused': False, 'terminated': False}
        
        try:
            task.status = 'running'
            db.session.commit()
            
            # æ¸…ç©ºä¹‹å‰çš„æ•°æ®ï¼Œé¿å…IDé‡å¤
            data_manager.translation_pairs.clear()
            
            # åŠ è½½ç¿»è¯‘æ•°æ®ï¼ˆå…¼å®¹è‹±->ä¸­æµ‹è¯•é›†ï¼‰
            data_manager.load_translation_pairs_from_json(filepath)
            translation_pairs = list(data_manager.translation_pairs.values())
            
            # æ ¹æ®æ•°æ®é€‰æ‹©å‚æ•°ç­›é€‰æ•°æ®
            if data_selection:
                translation_pairs = apply_data_selection(translation_pairs, data_selection)
            
            # é…ç½®ç¿»è¯‘å¼•æ“
            for model_key in selected_models:
                if model_key in config_manager.translation_models:
                    model_config = config_manager.translation_models[model_key]
                    translation_engine.add_model(model_key, model_config)
            
            # åˆå§‹åŒ–è¯„ä¼°å¼•æ“
            if config_manager.evaluation_model:
                # ç¡®ä¿ä¼ é€’æ­£ç¡®çš„ModelConfigå¯¹è±¡
                eval_config = config_manager.evaluation_model
                if hasattr(eval_config, '__dict__'):
                    # å¦‚æœæ˜¯ModelConfigå¯¹è±¡ï¼Œç›´æ¥ä½¿ç”¨
                    evaluation_engine = EvaluationEngine(eval_config)
                else:
                    # å¦‚æœæ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸ºModelConfigå¯¹è±¡
                    from translation_evaluation_config import ModelConfig
                    eval_config_dict = eval_config if isinstance(eval_config, dict) else eval_config.__dict__
                    evaluation_engine = EvaluationEngine(ModelConfig(**eval_config_dict))
            else:
                raise Exception("è¯„ä¼°æ¨¡å‹æœªé…ç½®")
            
            # æ‰§è¡Œç¿»è¯‘å’Œè¯„ä¼°
            all_results = []
            total_pairs = len(translation_pairs)
            
            for i, pair in enumerate(translation_pairs):
                try:
                    # æ£€æŸ¥ä»»åŠ¡æ§åˆ¶æ ‡å¿—
                    control_flags = task_control_flags.get(task_id, {})
                    
                    # æ£€æŸ¥æ˜¯å¦è¢«ç»ˆæ­¢
                    if control_flags.get('terminated', False):
                        task.status = 'terminated'
                        task.error_message = "ä»»åŠ¡è¢«ç”¨æˆ·ç»ˆæ­¢"
                        db.session.commit()
                        return
                    
                    # æ£€æŸ¥æ˜¯å¦è¢«æš‚åœ
                    while control_flags.get('paused', False):
                        task.status = 'paused'
                        db.session.commit()
                        time.sleep(1)  # ç­‰å¾…1ç§’åé‡æ–°æ£€æŸ¥
                        control_flags = task_control_flags.get(task_id, {})
                        
                        # åœ¨æš‚åœæœŸé—´ä¹Ÿè¦æ£€æŸ¥ç»ˆæ­¢
                        if control_flags.get('terminated', False):
                            task.status = 'terminated'
                            task.error_message = "ä»»åŠ¡åœ¨æš‚åœæœŸé—´è¢«ç»ˆæ­¢"
                            db.session.commit()
                            return
                    
                    # æ¢å¤è¿è¡ŒçŠ¶æ€
                    if task.status == 'paused':
                        task.status = 'running'
                        db.session.commit()
                    
                    # æ›´æ–°è¿›åº¦
                    progress = int((i / total_pairs) * 100)
                    task.progress = progress
                    db.session.commit()
                    
                    # å¯¹æ¯ä¸ªé€‰æ‹©çš„æ¨¡å‹è¿›è¡Œç¿»è¯‘
                    for model_key in selected_models:
                        # æ‰§è¡Œç¿»è¯‘ï¼ˆè¿”å› TranslationResult å¯¹è±¡ï¼‰
                        translation_result = translation_engine.translate_single(
                            model_key,
                            pair
                        )
                        
                        # æ‰§è¡Œè¯„ä¼°ï¼ˆè¿”å› EvaluationResult å¯¹è±¡ï¼‰
                        evaluation_result = evaluation_engine.evaluate_single(
                            pair,
                            translation_result
                        )
                        
                        # ä¿å­˜ç»“æœåˆ°æ•°æ®åº“
                        result = TranslationResult(
                            task_id=task_id,
                            pair_id=pair.id,
                            source_text=pair.source_text,
                            target_text=pair.target_text,
                            model_name=model_key,
                            translated_text=translation_result.translated_text,
                            accuracy_score=evaluation_result.accuracy_score,
                            fluency_score=evaluation_result.fluency_score,
                            terminology_score=evaluation_result.terminology_score,
                            overall_score=evaluation_result.overall_score,
                            evaluation_details=json.dumps(evaluation_result.__dict__, ensure_ascii=False)
                        )
                        db.session.add(result)
                        all_results.append(result)
                
                except Exception as e:
                    logging.error(f"å¤„ç†ç¿»è¯‘å¯¹ {pair.id} æ—¶å‡ºé”™: {e}")
                    continue
            
            # ä¿å­˜æœ€ç»ˆç»“æœ
            results_filename = f"evaluation_results_{task_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            results_filepath = os.path.join('results', results_filename)
            
            # ç”Ÿæˆç»“æœæŠ¥å‘Š
            report_data = generate_evaluation_report(all_results, selected_models)
            with open(results_filepath, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            task.status = 'completed'
            task.progress = 100
            task.completed_at = datetime.utcnow()
            task.results_file = results_filepath
            db.session.commit()
            
        except Exception as e:
            # ä»»åŠ¡å¤±è´¥
            task.status = 'failed'
            task.error_message = str(e)
            db.session.commit()
            # è¯¦ç»†çš„é”™è¯¯æ—¥å¿—
            import traceback
            error_details = traceback.format_exc()
            logging.error(f"è¯„ä¼°ä»»åŠ¡ {task_id} å¤±è´¥: {e}")
            logging.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {error_details}")
            print(f"[DEBUG] è¯„ä¼°ä»»åŠ¡å¤±è´¥è¯¦æƒ…: {error_details}")

def generate_evaluation_report(results, models):
    """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
    report = {
        'summary': {
            'total_pairs': len(set(r.pair_id for r in results)),
            'models_tested': models,
            'evaluation_time': datetime.now().isoformat()
        },
        'model_performance': {},
        'detailed_results': []
    }
    
    # æŒ‰æ¨¡å‹ç»Ÿè®¡æ€§èƒ½
    for model in models:
        model_results = [r for r in results if r.model_name == model]
        if model_results:
            avg_accuracy = sum(r.accuracy_score or 0 for r in model_results) / len(model_results)
            avg_fluency = sum(r.fluency_score or 0 for r in model_results) / len(model_results)
            avg_terminology = sum(r.terminology_score or 0 for r in model_results) / len(model_results)
            avg_overall = sum(r.overall_score or 0 for r in model_results) / len(model_results)
            
            report['model_performance'][model] = {
                'average_accuracy': round(avg_accuracy, 2),
                'average_fluency': round(avg_fluency, 2),
                'average_terminology': round(avg_terminology, 2),
                'average_overall': round(avg_overall, 2),
                'total_translations': len(model_results)
            }
    
    # è¯¦ç»†ç»“æœ
    for result in results:
        report['detailed_results'].append({
            'pair_id': result.pair_id,
            'source_text': result.source_text,
            'target_text': result.target_text,
            'model_name': result.model_name,
            'translated_text': result.translated_text,
            'scores': {
                'accuracy': result.accuracy_score,
                'fluency': result.fluency_score,
                'terminology': result.terminology_score,
                'overall': result.overall_score
            }
        })
    
    return report

@app.route('/api/tasks/<task_id>')
def get_task_status(task_id):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    task = EvaluationTask.query.get_or_404(task_id)
    
    # è·å–ä»»åŠ¡çš„ç¿»è¯‘ç»“æœ
    results = TranslationResult.query.filter_by(task_id=task_id).all()
    
    task_data = {
        'id': task.id,
        'name': task.name,
        'status': task.status,
        'progress': task.progress,
        'created_at': task.created_at.isoformat() if task.created_at else None,
        'completed_at': task.completed_at.isoformat() if task.completed_at else None,
        'total_pairs': task.total_pairs,
        'error_message': task.error_message,
        'results_count': len(results)
    }
    
    # å¦‚æœä»»åŠ¡å®Œæˆï¼ŒåŒ…å«ç»“æœç»Ÿè®¡
    if task.status == 'completed' and results:
        # æŒ‰æ¨¡å‹åˆ†ç»„ç»“æœ
        model_stats = {}
        for result in results:
            if result.model_name not in model_stats:
                model_stats[result.model_name] = {
                    'count': 0,
                    'avg_accuracy': 0,
                    'avg_fluency': 0,
                    'avg_terminology': 0,
                    'avg_overall': 0
                }
            
            stats = model_stats[result.model_name]
            stats['count'] += 1
            stats['avg_accuracy'] += result.accuracy_score or 0
            stats['avg_fluency'] += result.fluency_score or 0
            stats['avg_terminology'] += result.terminology_score or 0
            stats['avg_overall'] += result.overall_score or 0
        
        # è®¡ç®—å¹³å‡å€¼
        for model, stats in model_stats.items():
            count = stats['count']
            stats['avg_accuracy'] = round(stats['avg_accuracy'] / count, 2)
            stats['avg_fluency'] = round(stats['avg_fluency'] / count, 2)
            stats['avg_terminology'] = round(stats['avg_terminology'] / count, 2)
            stats['avg_overall'] = round(stats['avg_overall'] / count, 2)
        
        task_data['model_statistics'] = model_stats
    
    return jsonify(task_data)

@app.route('/api/tasks/<task_id>/pause', methods=['POST'])
def pause_task(task_id):
    """æš‚åœä»»åŠ¡"""
    task = EvaluationTask.query.get_or_404(task_id)
    
    if task.status not in ['running']:
        return jsonify({'error': 'åªèƒ½æš‚åœæ­£åœ¨è¿è¡Œçš„ä»»åŠ¡'}), 400
    
    # è®¾ç½®æš‚åœæ ‡å¿—
    if task_id in task_control_flags:
        task_control_flags[task_id]['paused'] = True
    
    return jsonify({'success': True, 'message': 'ä»»åŠ¡æš‚åœè¯·æ±‚å·²å‘é€'})

@app.route('/api/tasks/<task_id>/resume', methods=['POST'])
def resume_task(task_id):
    """æ¢å¤ä»»åŠ¡"""
    task = EvaluationTask.query.get_or_404(task_id)
    
    if task.status not in ['paused']:
        return jsonify({'error': 'åªèƒ½æ¢å¤å·²æš‚åœçš„ä»»åŠ¡'}), 400
    
    # æ¸…é™¤æš‚åœæ ‡å¿—
    if task_id in task_control_flags:
        task_control_flags[task_id]['paused'] = False
    
    return jsonify({'success': True, 'message': 'ä»»åŠ¡æ¢å¤è¯·æ±‚å·²å‘é€'})

@app.route('/api/tasks/<task_id>/terminate', methods=['POST'])
def terminate_task(task_id):
    """ç»ˆæ­¢ä»»åŠ¡"""
    task = EvaluationTask.query.get_or_404(task_id)
    
    if task.status not in ['running', 'paused']:
        return jsonify({'error': 'åªèƒ½ç»ˆæ­¢æ­£åœ¨è¿è¡Œæˆ–æš‚åœçš„ä»»åŠ¡'}), 400
    
    # è®¾ç½®ç»ˆæ­¢æ ‡å¿—
    if task_id in task_control_flags:
        task_control_flags[task_id]['terminated'] = True
    
    return jsonify({'success': True, 'message': 'ä»»åŠ¡ç»ˆæ­¢è¯·æ±‚å·²å‘é€'})

@app.route('/api/models/test', methods=['POST'])
def test_models():
    """ä½¿ç”¨ç¤ºä¾‹ç¿»è¯‘å¯¹æµ‹è¯•é€‰ä¸­çš„æ¨¡å‹"""
    try:
        data = request.get_json()
        selected_models = data.get('selected_models', [])
        
        if not selected_models:
            return jsonify({'error': 'æœªé€‰æ‹©ä»»ä½•æ¨¡å‹'}), 400
        
        # è·å–ç¤ºä¾‹ç¿»è¯‘å¯¹ï¼ˆä½¿ç”¨æ•°æ®é›†ä¸­çš„ç¬¬ä¸€æ¡ï¼‰
        sample_pair = None
        try:
            # å°è¯•ä»æœ€è¿‘ä¸Šä¼ çš„æ–‡ä»¶è·å–ç¤ºä¾‹
            data_manager.translation_pairs.clear()
            
            # æŸ¥æ‰¾æœ€è¿‘çš„JSONæ•°æ®æ–‡ä»¶
            data_dir = Path(__file__).parent.parent / "data"
            json_files = list(data_dir.glob("*.json"))
            
            if json_files:
                # ä½¿ç”¨æœ€æ–°çš„JSONæ–‡ä»¶
                latest_file = max(json_files, key=lambda f: f.stat().st_mtime)
                data_manager.load_translation_pairs_from_json(str(latest_file))
                
                if data_manager.translation_pairs:
                    sample_pair = next(iter(data_manager.translation_pairs.values()))
        except Exception as e:
            app.logger.warning(f"åŠ è½½ç¤ºä¾‹æ•°æ®å¤±è´¥: {e}")
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç¤ºä¾‹æ•°æ®ï¼Œä½¿ç”¨å†…ç½®ç¤ºä¾‹
        if not sample_pair:
            from translation_data_manager import TranslationPair
            sample_pair = TranslationPair(
                id="test_sample",
                source_text="This chapter defines the aircraft dimensions and areas for maintenance operations.",
                target_text="æœ¬ç« å®šä¹‰äº†ç»´ä¿®æ“ä½œä¸­çš„é£æœºå°ºå¯¸å’ŒåŒºåŸŸã€‚",
                source_lang="en",
                target_lang="zh",
                category="technical_manual",
                difficulty="medium",
                context="èˆªç©ºç»´ä¿®æ‰‹å†Œ"
            )
        
        # é…ç½®ç¿»è¯‘å¼•æ“
        for model_key in selected_models:
            if model_key in config_manager.translation_models:
                model_config = config_manager.translation_models[model_key]
                translation_engine.add_model(model_key, model_config)
        
        # æµ‹è¯•æ¯ä¸ªæ¨¡å‹
        test_results = []
        for model_key in selected_models:
            try:
                start_time = time.time()
                
                # æ‰§è¡Œç¿»è¯‘æµ‹è¯•
                translation_result = translation_engine.translate_single(model_key, sample_pair)
                
                processing_time = time.time() - start_time
                
                # ç®€å•çš„è´¨é‡æ£€æŸ¥
                quality_status = "æ­£å¸¸"
                if translation_result.error_message:
                    quality_status = "é”™è¯¯"
                elif translation_result.translated_text == sample_pair.source_text:
                    quality_status = "æœªç¿»è¯‘"
                elif sample_pair.target_lang == "zh" and not any('\u4e00' <= c <= '\u9fff' for c in translation_result.translated_text):
                    quality_status = "è¯­è¨€é”™è¯¯"
                
                test_results.append({
                    'model': model_key,
                    'status': 'success' if not translation_result.error_message else 'error',
                    'translated_text': translation_result.translated_text,
                    'processing_time': round(processing_time, 2),
                    'error_message': translation_result.error_message,
                    'quality_status': quality_status
                })
                
            except Exception as e:
                test_results.append({
                    'model': model_key,
                    'status': 'error',
                    'translated_text': '',
                    'processing_time': 0,
                    'error_message': str(e),
                    'quality_status': 'è¿æ¥å¤±è´¥'
                })
        
        return jsonify({
            'success': True,
            'sample_text': {
                'source': sample_pair.source_text,
                'reference': sample_pair.target_text
            },
            'test_results': test_results
        })
        
    except Exception as e:
        app.logger.error(f"æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        return jsonify({'error': f'æ¨¡å‹æµ‹è¯•å¤±è´¥: {str(e)}'}), 500

@app.route('/api/tasks/<task_id>/results')
def get_task_results(task_id):
    """è·å–ä»»åŠ¡çš„è¯¦ç»†ç»“æœ"""
    task = EvaluationTask.query.get_or_404(task_id)
    results = TranslationResult.query.filter_by(task_id=task_id).all()
    
    results_data = []
    for result in results:
        results_data.append({
            'pair_id': result.pair_id,
            'source_text': result.source_text,
            'target_text': result.target_text,
            'model_name': result.model_name,
            'translated_text': result.translated_text,
            'accuracy_score': result.accuracy_score,
            'fluency_score': result.fluency_score,
            'terminology_score': result.terminology_score,
            'overall_score': result.overall_score
        })
    
    return jsonify(results_data)

@app.route('/api/tasks')
def get_all_tasks():
    """è·å–æ‰€æœ‰ä»»åŠ¡åˆ—è¡¨"""
    tasks = EvaluationTask.query.order_by(EvaluationTask.created_at.desc()).all()
    
    tasks_data = []
    for task in tasks:
        tasks_data.append({
            'id': task.id,
            'name': task.name,
            'status': task.status,
            'progress': task.progress,
            'created_at': task.created_at.isoformat() if task.created_at else None,
            'completed_at': task.completed_at.isoformat() if task.completed_at else None,
            'total_pairs': task.total_pairs
        })
    
    return jsonify(tasks_data)

@app.route('/api/logs')
def get_logs():
    """è·å–å®æ—¶æ—¥å¿—"""
    limit = request.args.get('limit', 50, type=int)
    log_type = request.args.get('type', None)
    
    logs = log_manager.get_recent_logs(limit=limit, log_type=log_type)
    return jsonify(logs)

@app.route('/api/logs/translation/<task_id>')
def get_translation_logs(task_id):
    """è·å–ç‰¹å®šä»»åŠ¡çš„ç¿»è¯‘æ—¥å¿—"""
    logs = log_manager.get_recent_logs(log_type='translation')
    task_logs = [log for log in logs if log.get('task_id') == task_id]
    return jsonify(task_logs)

@app.route('/api/performance')
def get_performance_stats():
    """è·å–æ€§èƒ½ç»Ÿè®¡"""
    stats = performance_monitor.get_current_stats()
    return jsonify(stats)

@app.route('/api/performance/comparison')
def get_speed_comparison():
    """è·å–é€Ÿåº¦å¯¹æ¯”æ•°æ®"""
    comparison = performance_monitor.get_speed_comparison()
    return jsonify(comparison)

@app.route('/api/performance/start')
def start_performance_monitoring():
    """å¯åŠ¨æ€§èƒ½ç›‘æ§"""
    performance_monitor.start_monitoring()
    return jsonify({'status': 'started'})

@app.route('/api/performance/stop')
def stop_performance_monitoring():
    """åœæ­¢æ€§èƒ½ç›‘æ§"""
    performance_monitor.stop_monitoring()
    return jsonify({'status': 'stopped'})

@app.route('/api/config/reload', methods=['POST'])
def reload_config_from_env():
    """ä» .env é‡è½½åœ¨çº¿æ¨¡å‹ API Key å¹¶æŒä¹…åŒ–åˆ°é…ç½®æ–‡ä»¶"""
    try:
        if not config_manager:
            return jsonify({'error': 'é…ç½®ç®¡ç†å™¨æœªåˆå§‹åŒ–'}), 500
        dotenv_path = Path(__file__).parent.parent / '.env'
        load_dotenv(dotenv_path=dotenv_path)
        ernie_key = os.getenv('ERNIE_API_KEY', '').strip()
        qwen_key = os.getenv('QWEN_API_KEY', '').strip()
        qwen_base = os.getenv('QWEN_BASE_URL', '').strip()
        qwen_model = os.getenv('QWEN_MODEL_ID', '').strip()
        changed = False
        if 'ernie-4.5-0.3b' in config_manager.translation_models and ernie_key:
            config_manager.translation_models['ernie-4.5-0.3b'].api_key = ernie_key
            changed = True
        if 'qwen3-8b' in config_manager.translation_models:
            if (qwen_key or ernie_key):
                config_manager.translation_models['qwen3-8b'].api_key = (qwen_key or ernie_key)
                changed = True
            if qwen_base:
                config_manager.translation_models['qwen3-8b'].base_url = qwen_base
                changed = True
            if qwen_model:
                config_manager.translation_models['qwen3-8b'].model_id = qwen_model
                changed = True
        if config_manager.evaluation_model and ernie_key:
            config_manager.evaluation_model.api_key = ernie_key
            changed = True
        if changed:
            config_manager.save_config()
        return jsonify({'success': True, 'changed': changed})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/logs/clear')
def clear_logs():
    """æ¸…ç©ºæ—¥å¿—"""
    log_manager.clear_logs()
    return jsonify({'status': 'cleared'})

if __name__ == '__main__':
    # åˆå§‹åŒ–ç³»ç»Ÿ
    if initialize_system():
        print("ğŸš€ é£æœºç»´ä¿®ç¿»è¯‘è¯„ä¼°ç³»ç»Ÿ Webç‰ˆæœ¬å¯åŠ¨ä¸­...")
        print("ğŸ“± è®¿é—®åœ°å€: http://localhost:5001")
        print("âœ¨ åŠŸèƒ½ç‰¹æ€§: æ¨¡å‹é€‰æ‹©ã€å®æ—¶è¯„ä¼°ã€ç»“æœå±•ç¤ºã€ä»»åŠ¡æ§åˆ¶ã€æ¨¡å‹æµ‹è¯•")
        app.run(debug=True, host='0.0.0.0', port=5001)
    else:
        print("âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
